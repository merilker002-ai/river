import pandas as pd
import numpy as np
import json
import os
from datetime import datetime, timedelta
import warnings
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
import joblib

warnings.filterwarnings('ignore')

class AdaptiveSuTuketimModeli:
    def __init__(self, model_path="adaptive_model.joblib"):
        self.model_path = model_path
        self.learning_data = []
        
        # DAHA AKILLI BAÅLANGIÃ‡ THRESHOLD'LARI
        self.adaptive_thresholds = {
            'varyasyon_esik': 1.2,    # Daha hassas baÅŸla
            'yuksek_tuketim_esik': 40, # Daha dÃ¼ÅŸÃ¼k baÅŸla
            'trend_esik': 0.25,       # Daha hassas trend
            'sifir_esik': 1           # Daha hassas sÄ±fÄ±r tespiti
        }
        
        self.pattern_memory = {}
        self.performance_history = []
        
        # 1M+ SATIR Ä°Ã‡Ä°N BELLEK OPTÄ°MÄ°ZASYONU
        self.max_pattern_memory = 5000
        self.max_performance_history = 10000
        self.learning_batch_size = 500
        
        # OTOMATÄ°K Ã–ÄRENME VERÄ°SÄ°
        self._initialize_with_synthetic_data()
        self.load_model()
    
    def _initialize_with_synthetic_data(self):
        """Sentetik veri ile hemen Ã¶ÄŸrenmeye baÅŸla"""
        print("ğŸ¤– Sentetik veri ile AI eÄŸitiliyor...")
        
        # BaÅŸarÄ±lÄ± tahminler (gerÃ§ek hayattan beklenen pattern'ler)
        successful_patterns = [
            # Normal pattern'ler - DÃ¼ÅŸÃ¼k risk
            {'sifir_sayisi': 0, 'varyasyon': 0.5, 'trend': 0.05, 'tuketim': 15, 'risk': 'DÃ¼ÅŸÃ¼k'},
            {'sifir_sayisi': 0, 'varyasyon': 0.8, 'trend': 0.08, 'tuketim': 25, 'risk': 'DÃ¼ÅŸÃ¼k'},
            
            # Orta risk pattern'leri
            {'sifir_sayisi': 1, 'varyasyon': 1.1, 'trend': 0.15, 'tuketim': 35, 'risk': 'Orta'},
            {'sifir_sayisi': 0, 'varyasyon': 1.4, 'trend': 0.12, 'tuketim': 45, 'risk': 'Orta'},
            
            # YÃ¼ksek risk pattern'leri
            {'sifir_sayisi': 2, 'varyasyon': 1.8, 'trend': 0.35, 'tuketim': 60, 'risk': 'YÃ¼ksek'},
            {'sifir_sayisi': 3, 'varyasyon': 2.2, 'trend': 0.45, 'tuketim': 80, 'risk': 'YÃ¼ksek'},
        ]
        
        # Sentetik feedback'ler oluÅŸtur
        for pattern in successful_patterns:
            feedback = {
                'tesisat_no': f"SYNTHETIC_{hash(str(pattern))}",
                'gercek_durum': pattern['risk'],
                'tahmin_durum': pattern['risk'],  # DoÄŸru tahmin
                'tarih': datetime.now(),
                'basari': 1,
                'pattern': pattern
            }
            self.performance_history.append(feedback)
        
        print(f"âœ… {len(successful_patterns)} sentetik pattern ile AI eÄŸitildi")
    
    def load_model(self):
        """Ã–ÄŸrenilmiÅŸ modeli yÃ¼kler - daha gÃ¼Ã§lÃ¼ hata yÃ¶netimi"""
        try:
            if os.path.exists(self.model_path):
                model_data = joblib.load(self.model_path)
                self.adaptive_thresholds = model_data.get('adaptive_thresholds', self.adaptive_thresholds)
                self.pattern_memory = model_data.get('pattern_memory', {})
                
                # Mevcut performans geÃ§miÅŸine ekle (Ã§akÄ±ÅŸma olmasÄ±n)
                existing_history = model_data.get('performance_history', [])
                existing_ids = [p.get('tesisat_no') for p in self.performance_history]
                
                for item in existing_history:
                    if item.get('tesisat_no') not in existing_ids:
                        self.performance_history.append(item)
                
                # BELLEK OPTÄ°MÄ°ZASYONU - fazla veriyi kes
                if len(self.performance_history) > self.max_performance_history:
                    self.performance_history = self.performance_history[-self.max_performance_history:]
                
                print(f"âœ… Ã–ÄŸrenilmiÅŸ model yÃ¼klendi. Toplam gÃ¶zlem: {len(self.performance_history)}")
                
                # Threshold'larÄ± optimize et
                self.adaptive_learning()
                
        except Exception as e:
            print(f"âŒ Model yÃ¼klenemedi, sentetik veri ile devam: {e}")
    
    def save_model(self):
        """Modeli kaydeder - daha gÃ¼venli"""
        try:
            model_data = {
                'adaptive_thresholds': self.adaptive_thresholds,
                'pattern_memory': self.pattern_memory,
                'performance_history': self.performance_history[-self.max_performance_history:],  # Bellek optimizasyonu
                'last_update': datetime.now(),
                'version': '2.0-large-scale',
                'total_observations': len(self.performance_history)
            }
            joblib.dump(model_data, self.model_path)
            print(f"âœ… Model kaydedildi. Toplam gÃ¶zlem: {len(self.performance_history)}")
        except Exception as e:
            print(f"âŒ Model kaydedilemedi: {e}")
    
    def learn_from_feedback(self, tesisat_no, gercek_durum, tahmin_durum, pattern_data=None):
        """GeliÅŸmiÅŸ geri bildirimle Ã¶ÄŸrenme - BELLEK ODAKLI"""
        feedback = {
            'tesisat_no': tesisat_no,
            'gercek_durum': gercek_durum,
            'tahmin_durum': tahmin_durum,
            'tarih': datetime.now(),
            'basari': 1 if gercek_durum == tahmin_durum else 0,
            'pattern': pattern_data
        }
        
        # BELLEK KONTROLÃœ
        if len(self.performance_history) >= self.max_performance_history:
            # En eski %10 feedback'i sil
            delete_count = int(self.max_performance_history * 0.1)
            self.performance_history = self.performance_history[delete_count:]
        
        # Benzersiz feedback'leri ekle
        existing_ids = [p.get('tesisat_no') for p in self.performance_history]
        if tesisat_no not in existing_ids:
            self.performance_history.append(feedback)
        
        # TOPLU Ã–ÄRENME - Her 500 feedback'te bir
        if len(self.performance_history) % self.learning_batch_size == 0:
            self.adaptive_learning()
            self.save_model()
        
        print(f"ğŸ“ Yeni feedback: {tesisat_no} | GerÃ§ek: {gercek_durum} | Tahmin: {tahmin_durum}")
    
    def adaptive_learning(self):
        """Daha agresif adaptif Ã¶ÄŸrenme"""
        if len(self.performance_history) < 10:
            return
        
        # Son 500 kaydÄ± deÄŸerlendir
        evaluation_data = self.performance_history[-500:] if len(self.performance_history) > 500 else self.performance_history
        basari_orani = sum([p['basari'] for p in evaluation_data]) / len(evaluation_data)
        
        print(f"ğŸ¯ Ã–ÄŸrenme DeÄŸerlendirmesi: {len(evaluation_data)} gÃ¶zlem, BaÅŸarÄ±: {basari_orani:.1%}")
        
        # DAHA HIZLI Ã–ÄRENME
        learning_rate = 0.1  # Ã–ÄŸrenme hÄ±zÄ±nÄ± artÄ±r
        
        if basari_orani < 0.6:  # BaÅŸarÄ± dÃ¼ÅŸÃ¼kse threshold'larÄ± optimize et
            self.adaptive_thresholds['varyasyon_esik'] *= (1 - learning_rate)
            self.adaptive_thresholds['trend_esik'] *= (1 - learning_rate)
            self.adaptive_thresholds['yuksek_tuketim_esik'] *= (1 - learning_rate * 0.5)
            print("ğŸ”§ Threshold'lar sÄ±kÄ±laÅŸtÄ±rÄ±ldÄ± (dÃ¼ÅŸÃ¼k baÅŸarÄ±)")
            
        elif basari_orani > 0.85:  # BaÅŸarÄ± yÃ¼ksekse threshold'larÄ± gevÅŸet
            self.adaptive_thresholds['varyasyon_esik'] *= (1 + learning_rate)
            self.adaptive_thresholds['trend_esik'] *= (1 + learning_rate)
            self.adaptive_thresholds['yuksek_tuketim_esik'] *= (1 + learning_rate * 0.5)
            print("ğŸ”§ Threshold'lar gevÅŸetildi (yÃ¼ksek baÅŸarÄ±)")
        
        # Threshold'larÄ± makul sÄ±nÄ±rlarda tut
        self.adaptive_thresholds['varyasyon_esik'] = max(0.3, min(3.0, self.adaptive_thresholds['varyasyon_esik']))
        self.adaptive_thresholds['trend_esik'] = max(0.05, min(1.0, self.adaptive_thresholds['trend_esik']))
        self.adaptive_thresholds['yuksek_tuketim_esik'] = max(10, min(200, self.adaptive_thresholds['yuksek_tuketim_esik']))
        self.adaptive_thresholds['sifir_esik'] = max(1, min(5, self.adaptive_thresholds['sifir_esik']))
        
        print(f"ğŸ“Š Yeni Threshold'lar: {self.adaptive_thresholds}")
    
    def auto_learn_from_analysis(self, tesisat_verisi, analiz_sonucu):
        """Analiz sonuÃ§larÄ±ndan otomatik Ã¶ÄŸrenme - BELLEK ODAKLI"""
        if len(tesisat_verisi) < 6:  # Yeterli veri yoksa Ã¶ÄŸrenme
            return
        
        tuketimler = tesisat_verisi['AKTIF_m3'].values
        
        # Ã–ZET pattern oluÅŸtur (detaylÄ± veri yerine Ã¶zet)
        pattern_summary = {
            'sifir_sayisi': int(sum(tuketimler == 0)),
            'varyasyon': float(np.std(tuketimler) / np.mean(tuketimler)) if np.mean(tuketimler) > 0 else 0.0,
            'trend': float(self._calculate_trend(tuketimler)),
            'mean_tuketim': float(np.mean(tuketimler)),
            'max_tuketim': float(np.max(tuketimler)),
            'min_tuketim': float(np.min(tuketimler)),
            'okuma_sayisi': len(tuketimler)
        }
        
        # Pattern hash (daha az bellek)
        pattern_key = f"p_{hash(str(pattern_summary)) % 1000000}"
        
        # BELLEK KONTROLÃœ - ESKÄ° PATTERN'LERÄ° TEMÄ°ZLE
        self._clean_old_patterns()
        
        # Pattern'i hafÄ±zaya kaydet (sÄ±nÄ±rlÄ±)
        if len(self.pattern_memory) < self.max_pattern_memory:
            self.pattern_memory[pattern_key] = {
                'summary': pattern_summary,  # DetaylÄ± veri yerine Ã¶zet
                'risk_seviyesi': analiz_sonucu['risk_seviyesi'],
                'count': self.pattern_memory.get(pattern_key, {}).get('count', 0) + 1,
                'last_seen': datetime.now().timestamp()  # DateTime yerine timestamp
            }
        else:
            # En az kullanÄ±lan pattern'i sil
            self._remove_least_used_pattern()
            # Yeni pattern'i ekle
            self.pattern_memory[pattern_key] = {
                'summary': pattern_summary,
                'risk_seviyesi': analiz_sonucu['risk_seviyesi'],
                'count': 1,
                'last_seen': datetime.now().timestamp()
            }
    
    def _clean_old_patterns(self):
        """Eski ve kullanÄ±lmayan pattern'leri temizle"""
        if len(self.pattern_memory) > self.max_pattern_memory * 0.8:
            # 6 aydan eski pattern'leri sil
            cutoff_timestamp = (datetime.now() - timedelta(days=180)).timestamp()
            old_patterns = [
                key for key, pattern in self.pattern_memory.items()
                if pattern['last_seen'] < cutoff_timestamp
            ]
            for key in old_patterns:
                del self.pattern_memory[key]
            print(f"ğŸ§¹ {len(old_patterns)} eski pattern temizlendi")
    
    def _remove_least_used_pattern(self):
        """En az kullanÄ±lan pattern'i sil"""
        if not self.pattern_memory:
            return
        
        min_count_pattern = min(self.pattern_memory.items(), key=lambda x: x[1]['count'])
        del self.pattern_memory[min_count_pattern[0]]
        print("ğŸ§¹ En az kullanÄ±lan pattern silindi")
    
    def _calculate_trend(self, tuketimler):
        """Trend hesaplama"""
        if len(tuketimler) < 3:
            return 0
        return (tuketimler[-1] - tuketimler[0]) / tuketimler[0] if tuketimler[0] > 0 else 0

    def gelismis_davranis_analizi(self, tesisat_verisi):
        """GeliÅŸmiÅŸ davranÄ±ÅŸ analizi - Ã¶ÄŸrenme entegre"""
        if len(tesisat_verisi) < 3:
            return self._create_default_analysis("Yetersiz veri", "Orta", 0)
        
        tuketimler = tesisat_verisi['AKTIF_m3'].values
        tarihler = tesisat_verisi['OKUMA_TARIHI']
        
        # Ä°statistiksel Ã¶zellikler
        sifir_sayisi = sum(tuketimler == 0)
        sifir_orani = sifir_sayisi / len(tuketimler)
        std_dev = np.std(tuketimler) if len(tuketimler) > 1 else 0
        mean_tuketim = np.mean(tuketimler) if len(tuketimler) > 0 else 0
        varyasyon_katsayisi = std_dev / mean_tuketim if mean_tuketim > 0 else 0
        
        # Trend analizi
        trend_degeri = self._calculate_trend(tuketimler)
        
        # Ã–ÄRENÄ°LMÄ°Å THRESHOLD'LAR ile risk puanÄ± hesaplama
        risk_puan = self._calculate_adaptive_risk_score(
            sifir_sayisi, sifir_orani, varyasyon_katsayisi, 
            trend_degeri, mean_tuketim, len(tuketimler)
        )
        
        # Risk seviyesi belirleme
        risk_seviyesi = self._determine_risk_level(risk_puan)
        
        # ÅÃ¼pheli dÃ¶nem tespiti
        supheli_donemler = self._find_suspicious_periods(tuketimler, tarihler, sifir_sayisi)
        
        # Ã–ÄRENÄ°LMÄ°Å YORUM oluÅŸturma
        yorum = self._adaptive_yorum_olustur(
            risk_seviyesi, risk_puan, sifir_sayisi, 
            varyasyon_katsayisi, trend_degeri, mean_tuketim
        )
        
        # OTOMATÄ°K Ã–ÄRENME
        self.auto_learn_from_analysis(tesisat_verisi, {
            'risk_seviyesi': risk_seviyesi,
            'risk_puan': risk_puan,
            'yorum': yorum
        })
        
        return {
            'yorum': yorum,
            'supheli_donemler': supheli_donemler,
            'risk_seviyesi': risk_seviyesi,
            'risk_puan': risk_puan,
            'std_dev': std_dev,
            'mean_tuketim': mean_tuketim,
            'pattern_data': {
                'sifir_sayisi': sifir_sayisi,
                'varyasyon_katsayisi': varyasyon_katsayisi,
                'trend_degeri': trend_degeri,
                'mean_tuketim': mean_tuketim
            }
        }
    
    def _calculate_adaptive_risk_score(self, sifir_sayisi, sifir_orani, varyasyon_katsayisi, trend_degeri, mean_tuketim, data_length):
        """Adaptive risk skoru hesaplama"""
        risk_puan = 0
        
        # 1. SÄ±fÄ±r tÃ¼ketim analizi - adaptive threshold
        if sifir_sayisi >= self.adaptive_thresholds['sifir_esik']:
            risk_puan += 3
        elif sifir_sayisi == 1:
            risk_puan += 1
        
        if sifir_orani > 0.5:
            risk_puan += 2
        
        # 2. Varyasyon analizi - adaptive threshold
        varyasyon_esik = self.adaptive_thresholds['varyasyon_esik']
        if varyasyon_katsayisi > varyasyon_esik:
            risk_puan += 2
        elif varyasyon_katsayisi > varyasyon_esik * 0.7:
            risk_puan += 1
        
        # 3. Trend analizi - adaptive threshold  
        trend_esik = self.adaptive_thresholds['trend_esik']
        if abs(trend_degeri) > trend_esik:
            risk_puan += 2
        elif abs(trend_degeri) > trend_esik * 0.7:
            risk_puan += 1
        
        # 4. Anormal yÃ¼ksek tÃ¼ketim - adaptive threshold
        yuksek_tuketim_esik = self.adaptive_thresholds['yuksek_tuketim_esik']
        if mean_tuketim > yuksek_tuketim_esik:
            risk_puan += 2
        elif mean_tuketim > yuksek_tuketim_esik * 0.7:
            risk_puan += 1
        
        return risk_puan
    
    def _determine_risk_level(self, risk_puan):
        """Risk seviyesi belirleme"""
        if risk_puan >= 5:
            return "YÃ¼ksek"
        elif risk_puan >= 3:
            return "Orta"
        else:
            return "DÃ¼ÅŸÃ¼k"
    
    def _find_suspicious_periods(self, tuketimler, tarihler, sifir_sayisi):
        """ÅÃ¼pheli dÃ¶nemleri bul"""
        supheli_donemler = []
        if sifir_sayisi > 0:
            for idx in np.where(tuketimler == 0)[0]:
                if idx < len(tarihler):
                    try:
                        tarih_obj = pd.Timestamp(tarihler.iloc[idx])
                        supheli_donemler.append(tarih_obj.strftime('%m/%Y'))
                    except:
                        continue
        return ", ".join(supheli_donemler) if supheli_donemler else "Yok"
    
    def _adaptive_yorum_olustur(self, risk_seviyesi, risk_puan, sifir_sayisi, varyasyon_katsayisi, trend_degeri, mean_tuketim):
        """Adaptive yorum oluÅŸturma"""
        
        if risk_seviyesi == "YÃ¼ksek":
            yorumlar = []
            
            if sifir_sayisi >= self.adaptive_thresholds['sifir_esik']:
                yorumlar.append("DÃ¼zensiz sÄ±fÄ±r tÃ¼ketim paterni")
            if varyasyon_katsayisi > self.adaptive_thresholds['varyasyon_esik']:
                yorumlar.append("YÃ¼ksek tÃ¼ketim dalgalanmasÄ±")
            if abs(trend_degeri) > self.adaptive_thresholds['trend_esik']:
                yorumlar.append(f"{'YÃ¼kselen' if trend_degeri > 0 else 'DÃ¼ÅŸen'} tÃ¼ketim trendi")
            if mean_tuketim > self.adaptive_thresholds['yuksek_tuketim_esik']:
                yorumlar.append("Anormal yÃ¼ksek tÃ¼ketim")
            
            if yorumlar:
                return " | ".join(yorumlar) + " - Acil inceleme Ã¶nerilir"
            else:
                return "YÃ¼ksek riskli tÃ¼ketim paterni - Ä°nceleme gerekli"
        
        elif risk_seviyesi == "Orta":
            if sifir_sayisi == 1:
                return "Tekil sÄ±fÄ±r tÃ¼ketim - Ä°zleme gerektirir"
            elif varyasyon_katsayisi > self.adaptive_thresholds['varyasyon_esik'] * 0.7:
                return "Orta seviyede tÃ¼ketim dalgalanmasÄ±"
            else:
                return "TÃ¼ketim davranÄ±ÅŸÄ±nda kÃ¼Ã§Ã¼k tutarsÄ±zlÄ±klar"
        
        else:
            yorumlar = [
                "Normal tÃ¼ketim paterni",
                "Stabil tÃ¼ketim alÄ±ÅŸkanlÄ±ÄŸÄ±", 
                "TutarlÄ± tÃ¼ketim davranÄ±ÅŸÄ±"
            ]
            return np.random.choice(yorumlar)
    
    def _create_default_analysis(self, yorum, risk_seviyesi, risk_puan):
        """VarsayÄ±lan analiz oluÅŸtur"""
        return {
            'yorum': yorum,
            'supheli_donemler': "Yok",
            'risk_seviyesi': risk_seviyesi,
            'risk_puan': risk_puan,
            'std_dev': 0,
            'mean_tuketim': 0
        }
    
    def get_learning_stats(self):
        """DetaylÄ± Ã¶ÄŸrenme istatistiklerini getir"""
        if not self.performance_history:
            return {
                'toplam_gozlem': 0,
                'basari_orani': 0,
                'adaptive_thresholds': self.adaptive_thresholds,
                'model_version': '2.0-large-scale',
                'status': 'Sentetik veri ile baÅŸlatÄ±ldÄ±'
            }
        
        toplam_gozlem = len(self.performance_history)
        
        # GerÃ§ek feedback'leri filtrele (sentetik olmayanlar)
        real_feedbacks = [p for p in self.performance_history if not p.get('tesisat_no', '').startswith('SYNTHETIC_')]
        real_basari_orani = sum([p['basari'] for p in real_feedbacks]) / len(real_feedbacks) if real_feedbacks else 0
        
        # TÃ¼m feedback'ler
        total_basari_orani = sum([p['basari'] for p in self.performance_history]) / toplam_gozlem
        
        return {
            'toplam_gozlem': toplam_gozlem,
            'gercek_gozlem': len(real_feedbacks),
            'basari_orani': total_basari_orani,
            'gercek_basari_orani': real_basari_orani,
            'adaptive_thresholds': self.adaptive_thresholds,
            'model_version': '2.0-large-scale',
            'status': 'Aktif Ã¶ÄŸrenme modunda',
            'pattern_memory_size': len(self.pattern_memory)
        }

# Global adaptive model instance
adaptive_model = AdaptiveSuTuketimModeli()
